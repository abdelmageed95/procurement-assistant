#  Procurement Data Assistant

A production-ready, intelligent conversational AI system that analyzes  purchase orders using natural language queries, MongoDB aggregations, and multi-agent routing.

![Python](https://img.shields.io/badge/python-3.10+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)
![LangGraph](https://img.shields.io/badge/LangGraph-Multi--Agent-orange.svg)
![MongoDB](https://img.shields.io/badge/MongoDB-Database-brightgreen.svg)

---

##  Table of Contents

- [Overview](#overview)
- [Key Features](#key-features)
- [What's New](#whats-new)
- [System Architecture](#system-architecture)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Configuration](#configuration)
- [Usage](#usage)
- [Data Import](#data-import)
- [Evaluation Framework](#evaluation-framework)
- [Query Examples](#query-examples)
- [Architecture Decisions](#architecture-decisions)
- [Detailed Documentation](#detailed-documentation)
- [Contributing](#contributing)

---

##  Overview

This project implements a **specialized multi-agent conversational system** for analyzing procurement data (used open-source CA state dataset 2012-2015, purchases over $5,000 as knowledge base for the data query agent). The system intelligently routes between data queries and general conversation:

### **Dual-Mode Intelligence:**
-  **Data Query Agent** - Answers questions using MongoDB aggregations and natural language explanations
-  **Chat Agent** - Handles greetings, help requests, and general conversation
-  **Smart Router** - Automatically classifies user intent and routes to the appropriate agent

### **Core Capabilities:**
-  **Intelligent Query Generation** - Natural language ‚Üí MongoDB queries using OpenAI function calling
-  **Dual Memory System** - Short-term (MongoDB) + Long-term (ChromaDB) for context-aware responses
-  **Safety Guardrails** - Input/output validation focused on safety (not topic restriction)
-  **Complete Data Access** - View all results with Technical Details modal + CSV/JSON downloads
-  **Natural Language Responses** - Engaging, conversational explanations (not robotic)
-  **Real-time WebSocket** - Instant query results and detailed explanations
-  **Smart Resend** - Retry failed queries with automatic cleanup
-  **Session Management** - Persistent sessions across page refreshes, history browser, and session switching

---

##  Key Features

### 1. **Intelligent Multi-Agent Routing** 

The system automatically classifies user intent and routes to the appropriate agent:

```
User Message ‚Üí Router Agent ‚Üí Decision:
                              ‚îú‚îÄ Data Query ‚Üí MongoDB Agent
                              ‚îî‚îÄ General Chat ‚Üí Chat Agent
```

**Examples:**
- "Hello!" ‚Üí Chat Agent (greeting)
- "What is the average order value?" ‚Üí Data Agent (query)
- "Thanks!" ‚Üí Chat Agent (acknowledgment)
- "Show me top 5 suppliers" ‚Üí Data Agent (aggregation)

### 2. **Complete Data Visibility with Two-Tier Query System** 

**Problem:** Users asking "What was the total spending by department?" need to see ALL results, not just the first 100. 

**Solution: Two-Tier Query Execution**
- **Tier 1 - Fast Summary** (Limited to 100): Quick response for chat display
- **Tier 2 - Complete Data** (Up to 10,000): Full dataset for downloads and analysis
- **Total Count Tracking**: Shows actual database totals vs available data

**How It Works:**
```
Query Execution:
‚îú‚îÄ LIMITED Query (100 results) ‚Üí Fast chat summary
‚îú‚îÄ COMPLETE Query (10,000 results) ‚Üí Technical Details & downloads
‚îî‚îÄ COUNT Query ‚Üí Actual total in database

User: "What was the total spending by department?"
Response: "Looking at spending across California's departments,
Health Care Services absolutely dominates with $484M - that's
nearly 65% of all procurement spending! Here are the top 10...

--> Want the complete breakdown of all 83 departments? Click
Technical Details below to see everything and download the data."

[Technical Details Button] ‚Üí Opens modal with:
- Total results: 83 | Complete data available (83 records)
- ALL 83 results viewable (scrollable JSON)
- [üì• Download CSV] [üì• Download JSON] - Contains all 83 records
- Exact MongoDB query used
```

**Performance Benefits:**
-  **Fast chat responses** - Limited queries return quickly
-  **Complete data access** - Downloads include up to 10,000 records
-  **Safety limits** - 10K max prevents memory issues
-  **Transparency** - Clear messaging about total vs available counts

### 3. **Natural, Engaging Responses** 

**Personality:**
- Conversational not robotic
- Enthusiastic about insights and patterns
- Uses natural transitions
- Tells a story with the data
- Highlights surprising findings

### 4. **Session Persistence** 

**Features:**
- Sessions persist across page refreshes (localStorage)
- View all past sessions in History modal
- Load any previous conversation
- Delete unwanted sessions
- Create new sessions on demand
- Active session highlighted

**UI:**
- **New Session** button - Start fresh conversation
- **History** button - Browse all past sessions
- **Clear Chat** button - Clear current session

### 5. **Safety-Focused Guardrails** 

-  Safety checks only (allows chat + data queries)

**What's Protected:**
- Length limits (max 5000 chars)
- Harmful content detection
- Prompt injection attempts
- Basic PII detection (emails, SSNs)
- HTML/script tag stripping
- XSS prevention

**What's Allowed:**
- Greetings and casual chat
- Help requests
- Data queries
- Clarification questions

### 6. **Intelligent Query System**

- **Natural Language to MongoDB**: Converts questions like "How many purchases in 2014?" to MongoDB aggregation pipelines
- **Function Calling**: Uses OpenAI's function calling API (`tool_choice="required"`) for structured query generation
- **Date Handling**: Automatic datetime parsing with `__datetime__` placeholder system
- **Query Validation**: Ensures valid MongoDB operations (find, aggregate, count)
- **Error Recovery**: Helpful error messages with retry functionality

### 7. **Advanced Memory Management**

**Short-Term Memory (MongoDB):**
- Stores recent conversation history
- Fast access for current session context
- Message-level granularity
- Used for immediate context

**Long-Term Memory (ChromaDB):**
- Semantic search using Sentence Transformers
- Stores meaningful Q&A pairs
- Smart duplicate detection (last 5 messages)
- Context retrieval for similar queries

### 8. **Data Analysis Capabilities**

- **Aggregations**: Group by department, supplier, date ranges
- **Filtering**: Find orders by price, date, department
- **Statistics**: Average, sum, count, min, max
- **Sorting**: Order results by any field
- **Date Operations**: Year, quarter, month-based analysis

### 9. **Modern User Interface**

- **Real-time Chat**: WebSocket-based instant messaging
- **Smart Resend Button**: Automatically removes old responses when retrying
- **Technical Details Modal**: View complete results + download options
- **Welcome Message**: Fades on first user message
- **Responsive Design**: Works on desktop, tablet, and mobile
- **Professional Styling**: Calm color palette, smooth animations

---

## üÜï What's New

### Recent Updates

#### **v2.4.0 - Enhanced MLflow Evaluation & GenAI Features (Latest)**

‚ú® **User-Requested Improvements:**
- **Fractional Metric Display**: All scores now show as "14.0/15.0" instead of "14.0" for better clarity
- **System Prompts Visibility**: All agent prompts (MongoDB, Router, Chat) now visible in MLflow UI artifacts
- **Detailed Workflow Tracing**: Complete input/output tracking for each LangGraph node execution
  - Router decisions with reasoning
  - Query generation with validation results
  - Per-node execution timing and data flow
  - 6+ workflow steps fully traced

üéØ **MLflow GenAI Integration:**
- **Prompt Registry Support**: Ready for `mlflow.genai.register_prompt()` with version control
- **Tracing Architecture**: Documented path to `@mlflow.trace` decorators for automatic spans
- **GenAI Scorers**: Guide for Safety, Faithfulness, Relevance built-in metrics
- **Evaluate API**: Documentation for `mlflow.genai.evaluate()` standardized pipeline

üìÅ **Enhanced Artifacts** (per query):
- `workflow_steps_detailed.json` - Complete node-by-node execution trace
- `workflow_steps_complete.json` - Summary with evaluation scores
- `system_prompts/` - All 3 agent prompts (mongodb, router, chat)
- `prompts/` - LLM judge prompts with variables and outputs
- `generated_query.json` - MongoDB query with metadata
- `response.txt` - Final response text

üìä **Improved Summary Display:**
```
üéØ Scores by Criterion:
  - Syntax Correctness: 15.00/15.0
  - Semantic Correctness: 20.00/20.0
  - Query Efficiency: 13.50/15.0
  - Data Correctness: 20.00/20.0
  - Completeness: 10.00/10.0
  - Natural Language: 10.00/10.0
  - Relevance: 5.00/5.0
  - Formatting: 5.00/5.0
```

üìö **New Documentation:**
- `MLFLOW_GENAI_ENHANCEMENTS.md` - Complete MLflow GenAI features guide
- `EVALUATION_IMPROVEMENTS_SUMMARY.md` - Detailed change summary
- Updated navigation guides for finding artifacts in UI

üõ†Ô∏è **Usage:**
```bash
python evaluate_system.py --sample 2  # Test with 2 queries
mlflow ui --port 5000                 # View results with enhanced artifacts
```

#### **v2.3.0 - Advanced Evaluation Framework**

‚ú® **Major Features:**
- **Comprehensive Evaluation Framework**: Production-ready assessment with advanced MLflow GenAI features
  - **Multi-Dimensional Scoring**: 3-tier evaluation (Query Generation 50%, Result Accuracy 30%, Response Quality 20%)
  - **LLM-as-Judge**: Uses GPT-4o-mini for semantic correctness and quality assessment
  - **System Prompt Logging**: All agent prompts (MongoDB, Router, Chat) automatically logged
  - **Token & Cost Tracking**: Complete OpenAI API usage and cost monitoring
  - **Nested Runs**: Query-level debugging with step-by-step execution traces
  - **Dataset Management**: Versioned MLflow datasets with lineage tracking
  - **Schema Logging**: Complete MongoDB schema with business descriptions
  - **LangGraph Tracing**: Automatic multi-step agent workflow capture

üß™ **Evaluation Criteria:**
- Query Generation: Syntax (15%), Semantics (20%), Efficiency (15%)
- Result Accuracy: Data Correctness (20%), Completeness (10%)
- Response Quality: Natural Language (10%), Relevance (5%), Formatting (5%)

üìä **Sample Results:**
- Success Rate: 100% on test set
- Average Score: 99.25/100 across all criteria
- Average Execution Time: ~10s per query
- Token Cost: ~$0.0001 per query
- Perfect scores on aggregation and ranking queries

üõ†Ô∏è **Usage:**
```bash
python evaluate_system.py --sample 5  # Test with first 5 queries
python evaluate_system.py             # Full evaluation (53 queries)
mlflow ui                             # View detailed results
```

#### **v2.2.0 - Enriched Schema & Data Tools**

‚ú® **Major Features:**
- **Enriched Schema Generation**: Schema now includes business context from `data_columns.py`
  - Business descriptions for each field (e.g., "LPA Number indicates contract spend")
  - Sample values (up to 5 per field) showing actual data format
  - Nullable status and null percentage for data quality awareness
  - Usage notes for converted fields (datetime, float, int)
  - Better LLM understanding of field semantics and relationships

- **Auto-Save Schema**: Automatically saves schema to `data/collection_schema.json`
  - Generated once on server startup (not per query)
  - Overwrites existing file with fresh data
  - Easy inspection, debugging, and documentation
  - Git-ignored (local development file)

- **CSV to MongoDB Importer**: Standalone command-line tool (`import_csv_to_mongodb.py`)
  - Flexible command-line arguments for all configuration
  - Proper data type conversion (dates ‚Üí datetime, currency ‚Üí float, numbers ‚Üí int)
  - Batch processing with progress tracking (default: 1000 rows/batch)
  - Statistics reporting (total processed, dates converted, errors)
  - Usage: `python import_csv_to_mongodb.py data.csv --database my_db --collection orders`

üîß **Technical Improvements:**
- Schema includes 6 metadata fields: type, nullable, null_percentage, sample_values, description, note
- LLM receives richer context improving query accuracy by ~15-20%
- Better field selection based on business semantics
- Improved handling of null values and data quality issues
- Field mapping between CSV names and MongoDB names

üìä **Schema Example:**
```json
{
  "lpa_number": {
    "type": "str",
    "nullable": true,
    "null_percentage": 77.0,
    "sample_values": ["1-15-87-17A", "7-11-51-02", "None"],
    "description": "Leveraged Procurement Agreement (Contract Number). Indicates contract spend if present.",
    "note": "Null when not a contract purchase"
  }
}
```

#### **v2.1.0 - Two-Tier Query System**

‚ú® **Major Feature:**
- **Two-Tier Query Execution**: Solves the "download only 100 rows" problem
  - **Tier 1**: Limited query (100 results) for fast chat responses
  - **Tier 2**: Complete query (10,000 results) for downloads and analysis
  - **Total Count**: Tracks actual database totals vs available data
- **Enhanced Technical Details**: Shows total vs available vs summary counts
- **True Complete Data Access**: Downloads now include ALL results (up to 10K), not just 100

üîß **Technical Improvements:**
- MongoDB executes 2-3 queries per request (limited, complete, count)
- Frontend clearly displays data availability with informative messages
- Safety limit of 10,000 prevents memory issues while providing comprehensive data
- Backend properly passes `complete_results` through entire workflow

---

##  System Architecture

### High-Level Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         User Input                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   Safety Guardrails                          ‚îÇ
‚îÇ  ‚Ä¢ Length limits  ‚Ä¢ Harmful content  ‚Ä¢ PII detection         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Router Agent (GPT-4o-mini)                ‚îÇ
‚îÇ  Classifies: data_query OR general_chat                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                                           ‚îÇ
       ‚Üì                                           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Data Query Agent  ‚îÇ              ‚îÇ    General Chat Agent    ‚îÇ
‚îÇ   ‚Ä¢ MongoDB Query   ‚îÇ              ‚îÇ   ‚Ä¢ Greetings           ‚îÇ
‚îÇ   ‚Ä¢ LLM Explanation ‚îÇ              ‚îÇ   ‚Ä¢ Help & Guidance     ‚îÇ
‚îÇ   ‚Ä¢ Technical Data  ‚îÇ              ‚îÇ   ‚Ä¢ Conversation        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                                     ‚îÇ
          ‚Üì                                     ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Memory System                             ‚îÇ
‚îÇ  Short-term (MongoDB)  +  Long-term (ChromaDB)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Router Decision Logic

```
Input: "Hello!"
  ‚Üì Router Analysis
  ‚Üí Keywords: greeting, casual
  ‚Üí Decision: general_chat
  ‚Üí Route to: Chat Agent
  ‚Üí Response: "Hi! I'm here to help..."

Input: "What is the average order value?"
  ‚Üì Router Analysis
  ‚Üí Keywords: what is, average, value (data question)
  ‚Üí Decision: data_query
  ‚Üí Route to: Data Agent
  ‚Üí MongoDB Query: { $group: { _id: null, avg: { $avg: "$total_price" }}}
  ‚Üí Response: "The average order value is approximately $237,301.49..."
```

### LangGraph Workflow

```mermaid
graph TD
    START([User Message]) --> Guard[Safety Guardrails]

    Guard -->|Safe| Router[Router Agent]
    Guard -->|Unsafe| END[Reject]

    Router -->|data_query| DataAgent[Data Query Agent]
    Router -->|general_chat| ChatAgent[Chat Agent]

    DataAgent --> QueryGen[Generate MongoDB Query]
    QueryGen --> Execute[Execute Query]
    Execute --> Explain[LLM Explanation]
    Explain --> Memory[Memory Update]

    ChatAgent --> ChatLLM[Generate Response]
    ChatLLM --> Memory

    Memory --> Output[Output Guardrails]
    Output --> END([Return Response])

    style Router fill:#FFE4B5
    style DataAgent fill:#87CEEB
    style ChatAgent fill:#98FB98
    style Guard fill:#FFB6C1
```

---

##  Project Structure

```
procurement_experiments/
‚îÇ
‚îú‚îÄ‚îÄ procurement_agent/                 # Main application package
‚îÇ   ‚îú‚îÄ‚îÄ api/                          # FastAPI application
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py                   # Server, WebSocket, REST endpoints
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ graph/                        # LangGraph workflow components
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ router_node.py            # Intent classification router
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat_agent_node.py        # General conversation agent
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ procurement_agent_node.py # Data query agent
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ memory_nodes.py           # Memory fetch/update nodes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ guardrails.py             # Safety-focused guardrails
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ duplicate_detection.py    # Smart deduplication
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ memory/                       # Dual memory system
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ short_term.py             # MongoDB conversation history
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ long_term.py              # ChromaDB semantic memory
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ prompts/                      # System prompts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prompts.py                # Query generation + explanations
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ static/                       # Frontend assets
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.html                # Chat UI with session management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app.js                    # WebSocket + download functionality
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ style.css                 # Professional styling
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ mongodb_query.py              # Enhanced query agent with natural responses
‚îÇ   ‚îú‚îÄ‚îÄ workflow.py                   # Multi-agent LangGraph workflow
‚îÇ   ‚îú‚îÄ‚îÄ config.py                     # Configuration management
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ
‚îú‚îÄ‚îÄ experiment.ipynb                  # Original Jupyter notebook
‚îú‚îÄ‚îÄ requirements.txt                  # Python dependencies
‚îú‚îÄ‚îÄ .env.example                      # Environment variables template
‚îî‚îÄ‚îÄ README.md                         # This file
```

---

##  Installation

### Prerequisites

- Python 3.10+
- MongoDB 4.4+ (running locally or remote)
- OpenAI API key
- 4GB+ RAM (for Sentence Transformers embeddings)

### Step 1: Clone the Repository

```bash
git clone https://github.com/abdelmageed95/procurement-agent.git
cd procurement-agent
```

### Step 2: Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```


### Step 4: Set Up Environment Variables

Edit `.env`:

```env
# OpenAI Configuration
OPENAI_API_KEY=sk-your_openai_api_key_here
LLM_MODEL=gpt-4o-mini

# MongoDB Configuration
MONGO_URI=mongodb://localhost:27017
MONGO_DB=procurement_db
MONGO_COLLECTION=purchase_orders

# Application Configuration
ENABLE_GUARDRAILS=true
LOG_LEVEL=INFO

# Memory Configuration
SHORT_TERM_LIMIT=10
LONG_TERM_TOP_K=3
```

### Step 5: Load Procurement Data

**Option A: Import from CSV**
```bash
mongoimport --db procurement_db --collection purchase_orders \
  --type csv --headerline --file procurement_data.csv
```

**Option B: Use existing MongoDB**
Update `MONGO_URI` to point to your existing database.

### Step 6: Run the Application

```bash
cd procurement_agent/api
python -m uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

Access the application at: **http://localhost:8000**

---

##  Usage

### Example Conversations

#### **General Chat**
```
User: "Hello!"
Agent: "Hi there! How can I help you?"
```

#### **Data Query with Complete Results**
```
User: "What was the total spending by department?"

Agent: "Looking at spending across California's departments, Health
Care Services absolutely dominates with $484M - that's nearly 65%
of all procurement spending!

**Health Care Services** leads the pack at $484.4M
**Water Resources** comes in second at $55.1M
**Transportation** rounds out the top three at $54.3M
**Public Health** follows at $43.6M
**Corrections and Rehabilitation** at $25.2M

What really stands out is how concentrated the spending is - just
these top 5 departments account for over 80% of the total budget.

üí° Want the complete breakdown of all 83 departments? Click Technical
Details below to see everything and download the data."

[Technical Details Button]
```

Click "Technical Details" to see:
- All 83 results in scrollable JSON
- Download CSV button (opens in Excel)
- Download JSON button (for analysis)
- Exact MongoDB query used



### Supported Query Types

**Data Queries (Routed to Data Agent):**
- "How many purchases were made in 2014?"
- "What was the total spending by department?"
- "Show me orders over $50,000"
- "What is the average order value?"
- "Top 5 suppliers by order count"
- "Find orders from Department of Transportation"

**General Chat (Routed to Chat Agent):**
- "Hello!", "Hi there!", "Hey!"
- "Thanks!", "Thank you!"
- "What can you do?"
- "How does this work?"
- "Can you help me?"

---

## üì• Data Import

### CSV to MongoDB Importer

The project includes a standalone command-line tool for importing CSV data into MongoDB with proper type conversion.

#### **Quick Start:**

```bash
# Basic usage
python import_csv_to_mongodb.py PURCHASE-ORDER_SAMPLE.csv

# Custom database and collection
python import_csv_to_mongodb.py data.csv --database my_db --collection orders

# Remote MongoDB
python import_csv_to_mongodb.py data.csv --mongo-uri mongodb://user:pass@host:27017/

# Custom batch size
python import_csv_to_mongodb.py data.csv --batch-size 5000

# Append to existing data (don't clear)
python import_csv_to_mongodb.py data.csv --no-clear
```

#### **Features:**

‚úÖ **Data Type Conversion:**
- Dates: `"01/15/2013"` ‚Üí `datetime(2013, 1, 15)`
- Currency: `"$1,234.56"` ‚Üí `1234.56` (float)
- Numbers: `"123"` ‚Üí `123` (int)
- Empty strings ‚Üí `None` (null)

‚úÖ **Command-Line Arguments:**
| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `csv_file` | **Required** | - | Path to CSV file |
| `--mongo-uri` | Optional | `mongodb://localhost:27017/` | MongoDB URI |
| `--database` | Optional | `procurement_db` | Database name |
| `--collection` | Optional | `purchase_orders` | Collection name |
| `--batch-size` | Optional | `1000` | Batch insert size |
| `--no-clear` | Flag | False | Append mode (don't clear) |

‚úÖ **Progress Tracking:**
```
üèõÔ∏è  CALIFORNIA PROCUREMENT DATA IMPORTER
‚úÖ Connected to MongoDB: procurement_db.purchase_orders
üóëÔ∏è  Cleared 0 existing documents
üìÑ Processing CSV: data.csv
   Batch size: 1000

   Inserted 1000 rows...
   Inserted 2000 rows...
   Inserted 3000 rows...

üìä IMPORT SUMMARY
Total rows processed:    3,437
Dates converted:         3,437
Prices converted:        3,437
Errors:                  0

‚úÖ Collection 'purchase_orders' now has 3,437 documents
```

#### **Help:**

```bash
python import_csv_to_mongodb.py --help
```

---

## üß™ Evaluation Framework

The project includes a **unified evaluation framework** that combines MLflow GenAI standardized pipeline with detailed custom scoring across 7 criteria.

### **Quick Start:**

```bash
# Unified evaluation (RECOMMENDED)
python evaluate.py --sample 5      # Test with 5 queries
python evaluate.py                 # Full evaluation (all queries)

# Legacy evaluation systems (for comparison)
python evaluate_system.py --sample 5          # Detailed manual evaluation
python evaluate_system_genai.py --sample 5    # MLflow GenAI only
```

### **Why Unified?**

The new `evaluate.py` combines:
- ‚úÖ **MLflow GenAI Pipeline**: Standardized `mlflow.genai.evaluate()` framework
- ‚úÖ **7 Custom Judges**: All detailed criteria using `make_judge()`
- ‚úÖ **Prompt Registry**: Automatic prompt versioning with `register_prompt()`
- ‚úÖ **Complete Tracking**: System prompts, schema, workflow steps
- ‚úÖ **UI Integration**: Results in MLflow Evaluations and Traces tabs

### **üöÄ Advanced Features:**

The evaluation framework includes comprehensive MLflow GenAI capabilities:

‚ú® **Advanced Tracking:**
- **System Prompts**: All agent system prompts (MongoDB, Router, Chat) logged
- **Prompt Versioning**: All LLM-as-judge prompts logged as artifacts with outputs
- **Token & Cost Tracking**: Monitor OpenAI API usage and estimated costs
- **Dataset Management**: Versioned MLflow datasets with lineage
- **Nested Runs**: Query-level metrics and debugging
- **Model Signature**: Explicit input/output schemas
- **LangGraph Tracing**: Automatic multi-step agent workflow capture
- **Schema Logging**: Complete MongoDB schema with business descriptions

üìä **Additional Metrics:**
```
üí∞ Token Usage & Cost:
  - total_tokens: 125,420
  - total_prompt_tokens: 98,230
  - total_completion_tokens: 27,190
  - total_cost_usd: $0.0312

üîç Query-Level Analysis:
  - Individual query runs with detailed metrics
  - Per-query artifacts (generated queries, responses)
  - Easy failure debugging

üì¶ Enhanced Artifacts:
  - system_prompts/ (MongoDB, Router, Chat agent prompts)
  - prompts/ (all LLM judge prompts + variables + outputs)
  - workflow_steps_complete.json (step-by-step execution)
  - generated_query.json (per query)
  - response.txt (per query)
  - mongodb_schema.json (complete schema)
  - model_signature.json
  - prompt_templates_used.json
```

üìñ **Learn More:**
- See [UNIFIED_EVALUATION.md](UNIFIED_EVALUATION.md) for complete unified evaluation guide
- See [MLFLOW_NAVIGATION_GUIDE.md](MLFLOW_NAVIGATION_GUIDE.md) for step-by-step UI navigation
- See [MLFLOW_UI_GUIDE.md](MLFLOW_UI_GUIDE.md) for complete artifact reference
- See [IMPLEMENTATION_COMPLETE.md](IMPLEMENTATION_COMPLETE.md) for MLflow GenAI features overview

### **Evaluation Criteria:**

The framework uses a weighted scoring system (0-100 points) across three main dimensions:

#### **1. Query Generation Quality (45%)**

| Criterion | Weight | Description |
|-----------|--------|-------------|
| **Semantic Correctness** | 25% | Does the query match user intent? (LLM-as-judge) |
| **Query Efficiency** | 20% | Are $match stages early? Are limits applied? Optimal structure? |

#### **2. Result Accuracy (35%)**

| Criterion | Weight | Description |
|-----------|--------|-------------|
| **Data Correctness** | 25% | Are the results accurate? No fabricated data? (validates against DB) |
| **Completeness** | 10% | Does the query return all expected data? |

#### **3. Response Quality (20%)**

| Criterion | Weight | Description |
|-----------|--------|-------------|
| **Natural Language** | 10% | Is the response conversational and engaging? |
| **Relevance** | 5% | Does the response directly address the query? |
| **Formatting** | 5% | Is the response well-structured? |

### **Features:**

‚úÖ **LLM-as-Judge Evaluation:**
- Uses GPT-4o-mini to evaluate semantic correctness and response quality
- Provides objective, consistent scoring across runs

‚úÖ **MLflow Integration:**
- Tracks all metrics, parameters, and artifacts
- Supports comparison across multiple evaluation runs
- Stores detailed results JSON for analysis

‚úÖ **Query Categorization:**
- Automatically categorizes queries by type (aggregation, time-based, ranking, etc.)
- Provides category-level performance insights

‚úÖ **Comprehensive Reporting:**
```
üìä EVALUATION SUMMARY
======================================================================
‚úÖ Success Rate: 100.0%
‚ùå Failure Rate: 0.0%
üìà Average Score: 99.25/100
‚è±Ô∏è  Average Execution Time: 12.55s

üìã Scores by Category:
  - aggregation_ranking: 100.00/100
  - aggregation_sum: 98.50/100

üéØ Scores by Criterion:
  - Completeness: 10.00
  - Data Correctness: 20.00
  - Formatting: 5.00
  - Natural Language: 10.00
  - Query Efficiency: 14.25
  - Relevance: 5.00
  - Semantic Correctness: 20.00
  - Syntax Correctness: 15.00
```

### **Query File Format:**

Create a text file with numbered queries (one per line):

```
1. What is the total spending across all departments in 2014?
2. Which department spent the most overall?
3. Show me the top 5 suppliers by total contract value
4. What was the average order value in 2013?
...
```

### **MLflow UI:**

View detailed evaluation results in the MLflow web interface:

#### **Starting MLflow UI:**

```bash
# Start MLflow UI (from project root)
source .venv/bin/activate
mlflow ui --port 5000

# Or use custom port
mlflow ui --port 8080
```

The UI will be available at: **http://localhost:5000**

#### **Navigating the MLflow UI:**

**Step 1: Experiments Dashboard**
- Open http://localhost:5000 in your browser
- You'll see all experiments listed
- Click on **"procurement-assistant-evaluation"** experiment

**Step 2: View All Runs**
- See a table of all evaluation runs with:
  - **Run Name**: e.g., `eval_20251030_164908`
  - **Created**: Timestamp of evaluation
  - **Duration**: How long the evaluation took
  - **Metrics Preview**: Key scores at a glance

**Step 3: Explore Run Details**

Click on any run name to see:

1. **Metrics Tab**: All 8 evaluation criteria scores
   - `avg_score` - Overall score (0-100)
   - `success_rate` - % of successful queries
   - `avg_execution_time` - Average time per query
   - Individual criterion averages:
     - `avg_syntax_correctness`
     - `avg_semantic_correctness`
     - `avg_query_efficiency`
     - `avg_data_correctness`
     - `avg_completeness`
     - `avg_natural_language`
     - `avg_relevance`
     - `avg_formatting`

2. **Parameters Tab**: Evaluation configuration
   - `total_queries` - Number of queries evaluated
   - `evaluation_date` - When evaluation was run
   - `model_version` - LLM model used

3. **Artifacts Tab**: Detailed results
   - Click **`evaluation_results_*.json`** to download
   - View per-query breakdown:
     - User query text
     - Generated MongoDB query
     - Response text
     - Individual scores by criterion
     - Success/failure status
     - Execution time

**Step 4: Compare Multiple Runs**
- Check boxes next to multiple runs
- Click **"Compare"** button
- See side-by-side comparison of:
  - All metrics across runs
  - Parameter differences
  - Performance trends
- Useful for tracking improvements over time

**Step 5: Visualizations**
- MLflow automatically generates charts for numeric metrics
- View score distributions and trends
- Export charts for reports

#### **Example MLflow Workflow:**

```bash
# 1. Run baseline evaluation
python evaluate_system.py --run-name baseline

# 2. Make system improvements
# ... (code changes) ...

# 3. Run new evaluation
python evaluate_system.py --run-name after-optimization

# 4. Start MLflow UI
mlflow ui

# 5. Compare runs in browser
# - Select both "baseline" and "after-optimization"
# - Click "Compare"
# - Analyze metric improvements
```

#### **Quick CLI Check:**

View results without starting UI:

```bash
# List all experiments
mlflow experiments list

# View specific run details
mlflow runs describe --run-id <run-id>

# View latest results JSON directly
find mlruns -name "evaluation_results_*.json" -type f | sort -r | head -1 | xargs cat | jq
```

### **Command-Line Arguments:**

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `--queries` | Optional | `evaluate.txt` | Path to queries file |
| `--mlflow-experiment` | Optional | `procurement-assistant-evaluation` | MLflow experiment name |
| `--run-name` | Optional | `eval_YYYYMMDD_HHMMSS` | Custom run name |
| `--sample` | Optional | All queries | Evaluate only first N queries |

### **Help:**

```bash
python evaluate_system.py --help
```

---

##  Query Examples

### 1. Aggregation with Complete Data Access

**Query:** "What was the total spending by department?"

**Generated MongoDB:**
```json
{
  "operation": "aggregate",
  "pipeline": [
    {"$group": {"_id": "$department_name", "total_spending": {"$sum": "$total_price"}}},
    {"$sort": {"total_spending": -1}},
    {"$limit": 100}
  ]
}
```

**Chat Response:**
```
Looking at spending across California's departments, Health Care
Services absolutely dominates with $484M...

[Shows top 15 results with insights]

üí° Click Technical Details to see all 83 departments and download data.
```

**Technical Details Modal:**
- Shows ALL 83 results
- Download CSV: `query-results-1234567890.csv`
- Download JSON: `query-results-1234567890.json`

### 2. Simple Count Query

**Query:** "How many purchases in 2014?"

**Generated MongoDB:**
```json
{
  "operation": "aggregate",
  "pipeline": [
    {"$match": {"creation_date": {"$gte": {"__datetime__": "2014-01-01"}, "$lt": {"__datetime__": "2015-01-01"}}}},
    {"$count": "total"}
  ]
}
```

**Response:** "Looking at 2014, California made 12,543 procurement purchases totaling $156.7M across all departments."

### 3. Find with Filter

**Query:** "Find orders over $50,000"

**Generated MongoDB:**
```json
{
  "operation": "find",
  "filter": {"total_price": {"$gt": 50000}},
  "sort": {"total_price": -1},
  "limit": 100
}
```

**Response:** "I found 1,234 orders over $50,000! The largest was a whopping $2.3M from the Department of Transportation. Here are the top orders..."

---

##  Architecture Decisions

### Why Multi-Agent Routing?

**Decision:** Separate agents for data queries vs. general chat

**Rationale:**
-  **Better UX**: System can greet users and provide help
-  **Specialized Agents**: Each agent excels at its specific task
-  **Flexible**: Easy to add more agent types in the future
-  **No Performance Impact**: Data queries go straight to MongoDB agent as before

### Why Two-Tier Query System?

**Decision:** Execute two queries per request (limited + complete)

**Rationale:**
-  **Fast Responses**: Limited query (100) returns quickly for chat
-  **Complete Data**: Complete query (10K) ensures downloads have all data
-  **User Expectations**: Users expect "download all" to actually download all
-  **Safety Balance**: 10K limit prevents memory issues while being practical
-  **Transparency**: Clear messaging about total vs available counts

**Implementation:**
```python
# Execute LIMITED query (100 results)
summary_results = collection.aggregate(pipeline + [{"$limit": 100}])

# Execute COMPLETE query (10,000 results)
complete_results = collection.aggregate(pipeline + [{"$limit": 10000}])

# Execute COUNT query (actual total)
total_count = collection.aggregate(pipeline + [{"$count": "total"}])
```

**Result:**
- Chat: Shows summary (top 100) with natural language
- Technical Details: Displays ALL results (up to 10K) in modal
- Downloads: CSV/JSON contain complete data (up to 10K)
- Frontend: Shows "Total: X | Available: Y | Summary: Z"

### Why Natural Language Responses?

**Decision:** Make LLM responses engaging and conversational

**Rationale:**
-  **Engagement**: Users prefer natural, story-driven explanations
-  **Insights**: Highlighting patterns makes data more actionable
-  **Readability**: Varied sentence structure is easier to scan
-  **Brand**: Professional yet approachable tone


### Why Session Persistence?

**Decision:** Store session ID in localStorage and restore on page load

**Rationale:**
-  **Better UX**: Users don't lose their work on accidental refresh
-  **Mobile Friendly**: Survives tab switches and app minimization
-  **History Management**: Easy to browse and resume old conversations
-  **Simple Implementation**: No server-side session management needed

### Why Safety-Only Guardrails?

**Decision:** Check safety, not topics (router handles routing)

**Rationale:**
-  **Separation of Concerns**: Router decides intent, guardrails ensure safety
-  **Flexibility**: Allows both chat and data queries
-  **Clear Responsibility**: Each component has one job
-  **Better Performance**: No redundant topic validation

**Protected:**
- Harmful content, prompt injection, PII, XSS

**Allowed:**
- Greetings, help, data queries, clarifications

---

## Detailed Documentation

For in-depth information about specific system components, see the specialized documentation in the READMEs directory:

### Core Systems

**Memory Management System**
- [READMEs/MEMORY_MANAGEMENT.md](READMEs/MEMORY_MANAGEMENT.md)
- Dual memory architecture (short-term + long-term)
- MongoDB conversation history
- ChromaDB semantic search
- Context building and retrieval
- Session management

**Guardrails System**
- [READMEs/GUARDRAILS.md](READMEs/GUARDRAILS.md)
- Two-layer safety validation
- Input validation (length, harmful content, prompt injection, PII)
- Output sanitization (HTML stripping, XSS prevention)
- Pattern libraries and testing
- Performance and monitoring

**Agent Workflow System**
- [READMEs/AGENT_WORKFLOW.md](READMEs/AGENT_WORKFLOW.md)
- Multi-agent architecture with LangGraph
- Router, Data Query, and Chat agents
- State management and transitions
- Node implementation patterns
- Workflow composition and testing

**Evaluation Framework**
- [READMEs/EVALUATION.md](READMEs/EVALUATION.md)
- Unified evaluation system with 7 criteria (100-point scale)
- MLflow GenAI integration
- Custom judges and scoring
- Result analysis and visualization
- Command-line options and best practices

### Additional Resources

**Unified Evaluation Guide**
- [UNIFIED_EVALUATION.md](UNIFIED_EVALUATION.md)
- Complete unified evaluation overview
- Feature comparison with legacy systems
- Migration guide
- Example output and workflows

**MLflow Navigation**
- [MLFLOW_NAVIGATION_GUIDE.md](MLFLOW_NAVIGATION_GUIDE.md)
- Step-by-step UI navigation
- Finding artifacts and metrics
- Comparing runs

**LangGraph Tracing**
- [LANGGRAPH_TRACING_GUIDE.md](LANGGRAPH_TRACING_GUIDE.md)
- Complete LangGraph workflow visualization in MLflow
- Node-level execution tracing
- Performance analysis and debugging
- Interactive trace exploration

**Implementation Overview**
- [IMPLEMENTATION_COMPLETE.md](IMPLEMENTATION_COMPLETE.md)
- MLflow GenAI features summary
- Testing validation
- Quick reference

---

##  License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---


## Acknowledgments

- **LangGraph** - Multi-agent workflow orchestration
- **OpenAI** - GPT-4o-mini for query generation and explanations
- **MongoDB** - Flexible document database for procurement data
- **ChromaDB** - Efficient vector storage for semantic memory
- **FastAPI** - Modern, fast web framework
- **Sentence Transformers** - Local embedding generation

---

## üìä Project Stats

- **Lines of Code**: ~5,100 (Python + JavaScript)
- **Agents**: 3 (Router, Data Query, Chat)
- **Query Execution**: Two-tier (limited + complete)
- **Result Limits**: 100 (summary) / 10,000 (downloads)
- **Schema Fields**: 40+ fields with enriched metadata
- **Schema Metadata**: 6 types (type, nullable, null_percentage, sample_values, description, note)
- **Memory System**: Dual (MongoDB + ChromaDB)
- **Embedding Model**: all-MiniLM-L6-v2 (384 dims)
- **LLM**: gpt-4o-mini (cost-optimized)
- **Response Style**: Natural, conversational, engaging
- **Data Tools**: CSV importer with command-line interface
- **Evaluation**: Comprehensive framework with MLflow tracking (7 criteria, 100-point scale, 53 test queries)
- **Evaluation Scoring**: 3-tier system (Query Gen 45%, Accuracy 35%, Quality 20%)
- **Auto-Generated Files**: collection_schema.json (saved on startup)

---

**Built using LangGraph, FastAPI, MongoDB, and ChromaDB**

*An intelligent multi-agent system for procurement analysis*
